{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d3d2b8",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dad5cb",
   "metadata": {},
   "source": [
    "The script scrapes data from the https://thehackernews.com/ website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d821d",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728eba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import itertools\n",
    "\n",
    "#import time related packages\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "#import webscraping packages \n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "from selenium_stealth import stealth\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "#postgresql database packages \n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import csv\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d26f5",
   "metadata": {},
   "source": [
    "## Mine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6340ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the url \n",
    "# url = 'https://thehackernews.com/'\n",
    "attack_url = 'https://thehackernews.com/search/label/Cyber%20Attack'\n",
    "driver = uc.Chrome(use_subprocess=True)\n",
    "driver.get(attack_url)\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "# main_page_older = driver.find_element(By.CLASS_NAME, \"blog-pager-older-link-mobile\")\n",
    "# main_page_older.click()\n",
    "\n",
    "\n",
    "# next_mainpage_url = driver.current_url\n",
    "# print(f\"num:  url: {next_mainpage_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "826f3b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TimeoutException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisibility_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBlog1_blog-pager-older-link\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage is ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\web_scraping\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:81\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value:\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\web_scraping\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:125\u001b[0m, in \u001b[0;36mvisibility_of_element_located.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _element_if_visible(\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m StaleElementReferenceException:\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\web_scraping\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:857\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    855\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[1;32m--> 857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\web_scraping\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:435\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[0;32m    437\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\web_scraping\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mInvalidArgumentException\u001b[0m: Message: invalid argument: invalid locator\n  (Session info: chrome=104.0.5112.102)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00A878B3+2193587]\n\tOrdinal0 [0x00A20681+1771137]\n\tOrdinal0 [0x009341A8+803240]\n\tOrdinal0 [0x00962631+992817]\n\tOrdinal0 [0x0096273B+993083]\n\tOrdinal0 [0x0098F7C2+1177538]\n\tOrdinal0 [0x0097D7F4+1103860]\n\tOrdinal0 [0x0098DAE2+1170146]\n\tOrdinal0 [0x0097D5C6+1103302]\n\tOrdinal0 [0x009577E0+948192]\n\tOrdinal0 [0x009586E6+952038]\n\tGetHandleVerifier [0x00D30CB2+2738370]\n\tGetHandleVerifier [0x00D221B8+2678216]\n\tGetHandleVerifier [0x00B117AA+512954]\n\tGetHandleVerifier [0x00B10856+509030]\n\tOrdinal0 [0x00A2743B+1799227]\n\tOrdinal0 [0x00A2BB68+1817448]\n\tOrdinal0 [0x00A2BC55+1817685]\n\tOrdinal0 [0x00A35230+1856048]\n\tBaseThreadInitThunk [0x751FFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x770D7A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x770D7A6E+238]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mvisibility_of_element_located((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlog1_blog-pager-older-link\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX4\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage is ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mTimeoutException\u001b[49m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading took too much time!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TimeoutException' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((\"Blog1_blog-pager-older-link\", \"X4\")))\n",
    "    print(\"Page is ready!\")\n",
    "except TimeoutException:\n",
    "    print(\"Loading took too much time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a75c46",
   "metadata": {},
   "source": [
    "## Click all other pages under main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click the next page\n",
    "main_page_older = driver.find_element(By.ID, 'Blog1_blog-pager-older-link')\n",
    "main_page_older.click()\n",
    "time.sleep(4)\n",
    "\n",
    "next_mainpage_url = driver.current_url\n",
    "next_mainpage_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the main \n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "main_tag = soup.find_all('div', {'class': 'body-post clear'})\n",
    "# main_tag\n",
    "\n",
    "for i in main_tag:\n",
    "    \n",
    "    #get the data \n",
    "    main_header = i.find_all('h2')\n",
    "    main_header = [t.text.strip() for t in main_header][0]\n",
    "    \n",
    "    #get the dates \n",
    "    main_dates = i.find_all('div', class_ = 'item-label')\n",
    "    main_published_dates = [y.text.rstrip().split('\\ue804')[0].replace('\\ue802', '') for y in main_dates][0]\n",
    "\n",
    "    #get the author of the artile\n",
    "    main_author = [x.text.rstrip().split('\\ue804')[1] for x in main_dates][0]\n",
    "    \n",
    "    #get the summary of the details \n",
    "    main_sum_p = i.find_all('div', {'class': 'home-desc'})\n",
    "    main_sum_par = [z.text.rstrip().replace('\\xa0', ' ').replace('\\\\', '') for z in main_sum_p][0]\n",
    "    \n",
    "    #get the urls to full story \n",
    "    main_full_story_url = i.find_all('a', href = True) \n",
    "    main_full_story_url_url = [x['href'] for x in main_full_story_url]\n",
    "#     print(main_full_story_url_url)\n",
    "    \n",
    "    #get convert the lists into a dataframe\n",
    "    main_extracted_df = {'topics':main_header, 'dates':main_published_dates, 'author':main_author, 'links':main_full_story_url_url,'truncated content':main_sum_par}\n",
    "    \n",
    "    #turn the data into a dataframe\n",
    "    main_raw_df = pd.DataFrame(main_extracted_df)\n",
    "\n",
    "    #append the data into created dataframe\n",
    "    main_df = pd.concat([main_df, main_raw_df])\n",
    "    \n",
    "    \n",
    "#reset the index\n",
    "main_df.reset_index(drop = True, inplace = True)\n",
    "main_df['page'] = 'main page'\n",
    "\n",
    "#print the data\n",
    "main_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5566e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the url \n",
    "url = 'https://thehackernews.com/'\n",
    "\n",
    "driver = uc.Chrome(use_subprocess=True)\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f66a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeffe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the main \n",
    "main_df_2 = pd.DataFrame()\n",
    "\n",
    "for num in range(3):\n",
    "    print(f\"The number clicks: {num+1}\")\n",
    "    \n",
    "    #get the url \n",
    "    url = 'https://thehackernews.com/'\n",
    "\n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "\n",
    "    #click the next page\n",
    "    main_page_older = driver.find_element(By.ID,\"Blog1_blog-pager-older-link\")\n",
    "    main_page_older.click()\n",
    "    time.sleep(4)\n",
    "\n",
    "    next_mainpage_url = driver.current_url\n",
    "    print(f\"main page next url: {next_mainpage_url}\")\n",
    "\n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    driver.get(next_mainpage_url)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(3)\n",
    "\n",
    "    #generate a soup object\n",
    "    main_soup = bs.BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "\n",
    "\n",
    "    main_tag_2 = main_soup.find_all('div', {'class': 'body-post clear'})\n",
    "    # main_tag\n",
    "\n",
    "    for i in main_tag_2:\n",
    "\n",
    "        #get the data \n",
    "        main_header = i.find_all('h2')\n",
    "        main_header = [t.text.strip() for t in main_header][0]\n",
    "\n",
    "        #get the dates \n",
    "        main_dates = i.find_all('div', class_ = 'item-label')\n",
    "        main_published_dates = [y.text.rstrip().split('\\ue804')[0].replace('\\ue802', '') for y in main_dates][0]\n",
    "\n",
    "        #get the author of the artile\n",
    "        main_author = [x.text.rstrip().split('\\ue804')[1] for x in main_dates][0]\n",
    "\n",
    "        #get the summary of the details \n",
    "        main_sum_p = i.find_all('div', {'class': 'home-desc'})\n",
    "        main_sum_par = [z.text.rstrip().replace('\\xa0', ' ').replace('\\\\', '') for z in main_sum_p][0]\n",
    "\n",
    "        #get the urls to full story \n",
    "        main_full_story_url = i.find_all('a', href = True) \n",
    "        main_full_story_url_url = [x['href'] for x in main_full_story_url]\n",
    "    #     print(main_full_story_url_url)\n",
    "\n",
    "        #get convert the lists into a dataframe\n",
    "        main_extracted_df = {'topics':main_header, 'dates':main_published_dates, 'author':main_author, 'links':main_full_story_url_url,'truncated content':main_sum_par}\n",
    "\n",
    "        #turn the data into a dataframe\n",
    "        main_raw_df = pd.DataFrame(main_extracted_df)\n",
    "\n",
    "        #append the data into created dataframe\n",
    "        main_df_2 = pd.concat([main_df_2, main_raw_df])\n",
    "\n",
    "    \n",
    "#reset the index\n",
    "main_df_2.reset_index(drop = True, inplace = True)\n",
    "main_df_2['page'] = 'main page'\n",
    "\n",
    "#print the data\n",
    "main_df_2.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a soup object\n",
    "# soup_test = bs.BeautifulSoup(driver.page_source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af63485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "content_df = pd.DataFrame()\n",
    "\n",
    "#get the url \n",
    "url = 'https://thehackernews.com/'\n",
    "\n",
    "driver = uc.Chrome(use_subprocess=True)\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "#generate a soup object\n",
    "soup = bs.BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "tags = soup.find_all('div', {'class': 'body-post clear'})\n",
    "\n",
    "\n",
    "#get the headers links \n",
    "headers = soup.find_all('div', {'class': 'menu-box cf'})\n",
    "#create the place holder for links\n",
    "links = []\n",
    "\n",
    "#loop through the headers categories\n",
    "for i in headers:\n",
    "    row = i.find_all('li')\n",
    "    row = i.find_all('a', href = True)    \n",
    "    new_url = [x['href'] for x in row]\n",
    "    links.append(new_url)\n",
    "\n",
    "    #get the url\n",
    "    for x in links[0][1:5]:\n",
    "        base_url = url+x\n",
    "        page_focus = base_url.split('/')[-1].replace('%20', ' ')\n",
    "        print(f\"The page: {page_focus} url: {base_url}\")\n",
    "\n",
    "        #get the  page data\n",
    "        driver = uc.Chrome(use_subprocess=True)\n",
    "        driver.get(base_url)\n",
    "        driver.maximize_window()\n",
    "        time.sleep(5)\n",
    "        soup_headers = bs.BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "        #get the headers \n",
    "        header_tags = soup_headers.find_all('div', {'class', 'body-post clear'})\n",
    "\n",
    "        #loop through the page and get the data\n",
    "        for i in header_tags:\n",
    "\n",
    "            #get the urls to full story \n",
    "            full_story_url = i.find_all('a', href = True) \n",
    "            full_story_url = [x['href'] for x in full_story_url]\n",
    "\n",
    "            #get the topics \n",
    "            topics = i.find_all('h2')\n",
    "            topics = [x.text for x in topics]\n",
    "\n",
    "            #get the dates \n",
    "            dates = i.find_all('div', class_ = 'item-label')\n",
    "            published_dates = [y.text.rstrip().split('\\ue804')[0].replace('\\ue802', '') for y in dates]\n",
    "\n",
    "            #get the author of the artile\n",
    "            author = [x.text.rstrip().split('\\ue804')[1] for x in dates]\n",
    "\n",
    "            #get the summary of the details \n",
    "            sum_p = i.find_all('div', {'class': 'home-desc'})\n",
    "            sum_par = [z.text.rstrip().replace('\\xa0', ' ').replace('\\\\', '') for z in sum_p]\n",
    "\n",
    "                        \n",
    "            #get convert the lists into a dataframe\n",
    "            extracted_df = {'topics':topics, 'dates':published_dates, 'author':author, 'links':full_story_url,\n",
    "                           'truncated content':sum_par}\n",
    "\n",
    "            #turn the data into a dataframe\n",
    "            raw_df = pd.DataFrame(extracted_df)\n",
    "            raw_df['page'] = page_focus\n",
    "\n",
    "            #append the data into created dataframe\n",
    "            df = pd.concat([df, raw_df])\n",
    "    \n",
    " \n",
    "# #print the extacted data\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#print a sample of the data \n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "overal_df = pd.concat([main_df, df])\n",
    "overal_df.reset_index(drop = True, inplace = True)\n",
    "overal_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be92d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overal_df['links'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a content df \n",
    "con_df = pd.DataFrame()\n",
    "count = 0\n",
    "    \n",
    "#get the url \n",
    "for link in  overal_df['links']:\n",
    "    count +=1\n",
    "    print(f\"Website no: {count} link: {link}\")\n",
    "    \n",
    "    #load the page\n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "\n",
    "    #generate a soup object\n",
    "    content_soup = bs.BeautifulSoup(driver.page_source,'lxml')\n",
    "    \n",
    "    \n",
    "    for i in content_soup.find_all('div', {'class':'main-box clear'}):\n",
    "\n",
    "        #get the header\n",
    "        hd = i.find_all('h1')\n",
    "        header = [x.text.strip() for x in hd][0]\n",
    "        print(f\" {'*'*5} story: {header} {'*'*5}\")\n",
    "\n",
    "        #get the url \n",
    "        con_ls_df = {\"content\":[]}\n",
    "        links_ls_df = {\"links\":[]}\n",
    "        for p in i.find_all('p'):\n",
    "            con_ls = [x.text.strip() for x in p][0]\n",
    "            con_ls_df['content'].append(con_ls)\n",
    "            \n",
    "            #links \n",
    "            ik = p.find_all('a')\n",
    "            extra_links = [x['href'].strip() for x in ik]\n",
    "            \n",
    "            for lp in extra_links:\n",
    "                links_ls_df['links'].append(lp)\n",
    "            \n",
    "           \n",
    "\n",
    "        #get con frames \n",
    "        con_frames = {'topics':header, 'full story':con_ls_df, \"extra links\":links_ls_df}\n",
    "        con_frames_df = pd.DataFrame(con_frames)\n",
    "        con_df = pd.concat([con_df, con_frames_df])\n",
    "\n",
    "        \n",
    "        \n",
    "#clen the data \n",
    "con_df_1 = con_df[con_df['extra links'].isna()==True]\n",
    "con_df_1.drop(columns = ['extra links'], inplace = True, axis = 1)\n",
    "\n",
    "con_df_2 = con_df[con_df['extra links'].isna()==False]\n",
    "con_df_2.drop(columns = ['full story'], inplace = True, axis = 1)\n",
    "\n",
    "con_df = pd.merge(con_df_1, con_df_2, how = 'left', on = 'topics')\n",
    "con_df['full story'] = con_df['full story'].apply(lambda x: \"\".join(x))\n",
    "con_df['extra links'] = con_df['extra links'].apply(lambda x: \"; \".join(x))\n",
    "\n",
    "# reset index \n",
    "con_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "con_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(overal_df, con_df, how = 'left',on = 'topics')\n",
    "print(f\"The rows before droping the duplicates {data.shape[0]}\")\n",
    "data.drop_duplicates(subset = ['topics', 'dates', 'author', 'links'], inplace = True)\n",
    "print(f\"The rows after droping the duplicates {data.shape[0]}\")\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create a content df \n",
    "# con_df = pd.DataFrame()\n",
    "\n",
    "# for i in content_soup.find_all('div', {'class':'main-box clear'}):\n",
    "    \n",
    "#     #get the header\n",
    "#     hd = i.find_all('h1')\n",
    "#     header = [x.text.strip() for x in hd][0]\n",
    "    \n",
    "#     #get the url \n",
    "#     con_ls_df = {\"content\":[]}\n",
    "#     for p in i.find_all('p'):\n",
    "#         con_ls = [x.text.strip() for x in p][0]\n",
    "#         con_ls_df['content'].append(con_ls)\n",
    "# #         print(con_ls)\n",
    "    \n",
    "#     #get con frames \n",
    "#     con_frames = {'topic':header, 'full story':con_ls_df}\n",
    "#     con_frames_df = pd.DataFrame(con_frames)\n",
    "#     con_df = pd.concat([con_df, con_frames_df])\n",
    "    \n",
    "# # reset index \n",
    "# con_df.reset_index(drop = True, inplace = True)\n",
    "# con_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(con_ls_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con_ls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27afcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c174f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a filename \n",
    "name = f\"hacker_news_website\"\n",
    "\n",
    "#get todays date\n",
    "today = datetime.now()\n",
    "\n",
    "# format the dates\n",
    "formated_date = today.strftime(\"%d_%b\").lower()\n",
    "\n",
    "#extension\n",
    "extension = \".xlsx\"\n",
    "\n",
    "#generate the name\n",
    "new_filename = name+\"_\"+formated_date\n",
    "old_filename = name+\"_\"+formated_date+\"_\"+\"oldfile\"\n",
    "print(old_filename ,\"\\n\", new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461019be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the connection \n",
    "# table_nam\n",
    "db_host = 'research-dev-db.cluster-c30rc84jwwhk.us-east-2.rds.amazonaws.com'\n",
    "# db_host = '192.168.8.221'\n",
    "db_port = 5432\n",
    "db_password = 'researchpassword'\n",
    "db = 'postgres'\n",
    "db_user = 'postgres'\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db}')\n",
    "\n",
    "#write the data if the table exists , replace the data\n",
    "try:\n",
    "    data.to_sql(new_filename, engine, method=psql_insert_copy, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(f'The error is: {str(e)}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the file if is in directory \n",
    "try:\n",
    "  os.rename(old_filename+'.xlsx', new_filename+'xlsx')\n",
    "except Exception as e:\n",
    "  print(f\"The error is: {str(e)}\")\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the file\n",
    "try:\n",
    "    data.to_excel(new_filename+'.xlsx', sheet_name = 'tweets data', index = False)\n",
    "#     coordinates_df.to_excel(\"african cities coordinates\"+'.xlsx', sheet_name = 'tweets data', index = False)\n",
    "except BaseException as e:\n",
    "    print('-'*20)\n",
    "    print(f\"The error is {str(e)}\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
