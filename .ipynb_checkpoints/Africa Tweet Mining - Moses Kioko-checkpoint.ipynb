{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8hDA6zxpgqu"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "> The script automates gathering of tweet data from the archives\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/342441381"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqWbxVU5wD2S"
   },
   "outputs": [],
   "source": [
    "#import packages \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "import requests\n",
    "import itertools    \n",
    "import json\n",
    "import re\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#import data scraping packages \n",
    "import tweepy\n",
    "# from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import Cursor\n",
    "from tweepy import API\n",
    "\n",
    "#import web scrapping packages \n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "from selenium_stealth import stealth\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "\n",
    "\n",
    "#sns module \n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import snscrape.modules.facebook as snfacebook\n",
    "import snscrape.modules.instagram as sninstagram\n",
    "\n",
    "#postgresql database packages \n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "#check the start time\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSl4LEoodIWL"
   },
   "source": [
    "## Extract Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a4ur9LDdHju",
    "outputId": "60957d45-96d2-4e7d-cdcf-ad5abfb3f59a"
   },
   "outputs": [],
   "source": [
    "#generate the \n",
    "number_of_dates = 365*5\n",
    "end_date = date.today()\n",
    "start_date =  end_date - timedelta(days=number_of_dates)\n",
    "duration = end_date - start_date\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"The start date: {start_date} end date is: {end_date} and time duration is: {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwYP-zQ5IeCF",
    "outputId": "fad41d98-5e13-404b-e076-acd9d9556c0a"
   },
   "outputs": [],
   "source": [
    "#get todays date\n",
    "today = datetime.now()\n",
    "\n",
    "# format the dates\n",
    "formated_date = today.strftime(\"%B %d %Y %H %M %S\")\n",
    "\n",
    "#print date \n",
    "print()\n",
    "print()\n",
    "print(\"Todays date: \", formated_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39bJ7aMOu_Ds"
   },
   "source": [
    "### Keywords\n",
    "\n",
    "> The following are search keywords for cyber security concerns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySKqMbc7LbU5"
   },
   "outputs": [],
   "source": [
    "nigerian_slag = ['gbege','trabaye','gbemi landicap',' our cousins',' yarn opata',' dey don jonz me',' i don loose guard',\n",
    "                 ' won ti bo card','maga don pay',' money miss road',' mon te ke',' mumu me',' e don enter',' won ti gbami',\n",
    "                 ' won gba mi loju',' abaye','gbemi landicap',' our cousins',' yarn opata',' dey don jonz me',\n",
    "                 ' i don loose guard',' money miss road',' mon te ke',' mumu me',' e don enter',' won ti gbami',\n",
    "                 ' won gba mi loju',' yawa don gas',' normal level',' that format',' sinzu money',' aza',' bad market',\n",
    "                 'maga ti sanwo',' maaye ti gbowo wole',' osu ti sanwo',' client ti gbemi landicap',' mo fe trabaye bayi',\n",
    "                 'wired my moni',' don comot my money o',' e don shele',' yawa don gash',' gbege don start',' see gbege',\n",
    "                 ' see kasala',' wetin dey sub',' which wayyyy',' mugu don pay',' kasala don burst!','moti wo gau',\n",
    "                 ' mewa nsele',' owo jona',' owo wogbo',' ole gbe',' kele gbe',' won semi ni janba',' won fun mi ni format',\n",
    "                 '419',' wuruwuru',' gbajue',' one chance',' wonti gba',' won lu ni jibiti',' dey do mi wayo',' magomago',\n",
    "                 ' ole',' duped',' snatched',' skimming',' farb',' jaguda',' gbewiri',' dey don run streets',\n",
    "                 ' e reason me bad',' dey don use am for me',' dey don do me ajao.','don hack my account',' dey don kpokpo me',\n",
    "                 ' dey dagbo','dey don carry me',' peak me',' wire']\n",
    "\n",
    "#fornat the sland \n",
    "print()\n",
    "print()\n",
    "print(f\"{'*'*50} Keyword list {'*'*60}\")\n",
    "nigerian_slag = [x.strip().lower() for x in nigerian_slag]\n",
    "print(nigerian_slag)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "#search words \n",
    "print(f\"{'*'*50} Joined Keyword List {'*'*50}\")\n",
    "search_word = \"|\".join([j.lower() for j in nigerian_slag])\n",
    "print(search_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f9oeJDvu4F5"
   },
   "source": [
    "### African Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-saalAL9VJ7",
    "outputId": "886ecc54-7bb8-466c-d74e-27b4756f7d61"
   },
   "outputs": [],
   "source": [
    "#countries involved\n",
    "name = ['Algeria','Angola','Benin','Botswana','Burkina_Faso','Burundi','Cameroon','Cape_Verde',\n",
    "             'Central_African_Republic','Chad','Comoros','Democratic_Republic_of_the_Congo','Djibouti','Egypt',\n",
    "             'Equatorial_Guinea',' Eritrea',' Ethiopia',' Gabon',' Gambia',' Ghana','Guinea',' Guinea-Bissau',\n",
    "             'Ivory_Coast','Kenya',' Lesotho',' Liberia',' Libya',' Madagascar',' Malawi',' Mali',' Mauritania','Mauritius',\n",
    "             'Morocco',' Mozambique',' Namibia',' Niger',' Nigeria','Republic of the Congo',' Rwanda','Sao_Tome_and_Principe',\n",
    "             'Senegal',' Seychelles',' Sierra_Leone',' Somalia',' South_Africa',' Sudan','South_Sudan','Swaziland',' Tanzania',' Togo',\n",
    "             'Tunisia',' Uganda',' Zambia',' Zimbabwe']\n",
    "\n",
    "# #strip any white spaces \n",
    "country_name = [x.strip().lower().replace('_','-').replace(' ','-') for x in name]\n",
    "\n",
    "#search words \n",
    "countries = \"|\".join([j.strip().lower() for j in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "y8R4t7iRmd4R",
    "outputId": "415c399b-ca7a-4fe7-e2d9-672e2cadbb07"
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "e84__ErAko4b",
    "outputId": "40ca4129-ec3e-4472-c48c-c68ae11c2089"
   },
   "outputs": [],
   "source": [
    " driver = webdriver.Chrome(r'C:\\Users\\chromedriver_win32\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQvIVuH9chM1"
   },
   "source": [
    "## Place Specific \n",
    "\n",
    "> Countries towns and cities co-ordinates [link](https://www.countrycoordinate.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data in a dataframe \n",
    "coordinates_df =  pd.DataFrame()\n",
    "count = 0\n",
    "\n",
    "for country in country_name:\n",
    "    #get the url\n",
    "    url = f\"https://www.countrycoordinate.com/country-{country}/\"\n",
    "    count +=1\n",
    "    print(f\"Num: {count}; link: {url}\") \n",
    "    try:\n",
    "        driver = uc.Chrome(use_subprocess=True)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        soup = bs.BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "        # get data table & save as dataframe\n",
    "        table_val = soup.find(\"table\",{'table table-bordered table-hover'})\n",
    "        table_val_df = pd.read_html(str(table_val))[0]\n",
    "\n",
    "        table_val_df.columns = ['city', 'coords']\n",
    "        table_val_df['country'] = country\n",
    "        table_val_df['url'] = url\n",
    "        table_val_df[['lat', 'lon']] = table_val_df['coords'].str.split('/', expand = True)\n",
    "        table_val_df.drop(columns = ['coords'], inplace = True)\n",
    "        if table_val_df.shape[0]>1:\n",
    "            coordinates_df = pd.concat([coordinates_df,table_val_df])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        #get the number of other pages \n",
    "        page_num_table = soup.find_all(\"table\", {\"class\":\"table table-bordered centered\"})\n",
    "\n",
    "\n",
    "\n",
    "          #loop through other pages\n",
    "        print(f\"{'*'*15} base page done {'*'*15}\")\n",
    "        print(f\"{'*'*15} Other pages {'*'*15}\")\n",
    "        for tab in page_num_table:\n",
    "            row = tab.find_all('a')\n",
    "            row = [i.text.strip() for i in row]\n",
    "            row = row[:-1]\n",
    "            print(f\"{'*'*15} pages to extract {len(row)+1} {'*'*15}\")\n",
    "\n",
    "            for page in row: \n",
    "                next_page_url = url+f\"?page={page}\"\n",
    "                print(next_page_url)\n",
    "\n",
    "                #get the data \n",
    "                driver.get(next_page_url)\n",
    "                time.sleep(2)\n",
    "                soup_page = bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "                # get data table & save as dataframe\n",
    "                table_val_next = soup_page.find(\"table\",{'table table-bordered table-hover'})\n",
    "                table_val_df_1 = pd.read_html(str(table_val_next))[0]\n",
    "                table_val_df_1.columns = ['city', 'coords']\n",
    "                table_val_df_1['country'] = country\n",
    "                table_val_df_1['url'] = url\n",
    "                table_val_df_1[['lat', 'lon']] = table_val_df_1['coords'].str.split('/', expand = True)\n",
    "                table_val_df_1.drop(columns = ['coords'], inplace = True)\n",
    "                if table_val_df_1.shape[0]>1:\n",
    "                    coordinates_df = pd.concat([coordinates_df,table_val_df_1])\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'The error is: {str(e)}')\n",
    "        pass\n",
    "    \n",
    "#clean the data \n",
    "\n",
    "#reset the index\n",
    "coordinates_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#clean the data \n",
    "coordinates_df['city'] = coordinates_df['city'].str.split('(', expand = True)[0]\n",
    "\n",
    "#drop duplicate towns\n",
    "print(f\"Rows befor droping duplicates {coordinates_df.shape[0]}\")\n",
    "coordinates_df.drop_duplicates(subset = ['city', 'country'], inplace = True, keep = 'last')\n",
    "print(f\"Rows befor droping duplicates {coordinates_df.shape[0]}\")\n",
    "\n",
    "#print the sample \n",
    "coordinates_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Select Nigeria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nigeria = coordinates_df[coordinates_df['country'] == 'nigeria']\n",
    "nigeria.reset_index(drop = True, inplace = True)\n",
    "# coordinates_df\n",
    "nigeria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extract tweets with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and empty df\n",
    "df_x = pd.DataFrame()\n",
    "\n",
    "#define the max keywords\n",
    "maxcounts = 50\n",
    "count = 0\n",
    "\n",
    "try: \n",
    "    for row in nigeria.index:\n",
    "            country = nigeria['country'][row]\n",
    "            nearby_location = nigeria['city'][row]\n",
    "            lat = nigeria['lat'][row]\n",
    "            lon = nigeria['lon'][row]\n",
    "            circumfrence = \"20km\"\n",
    "            coor = lat+\",\"+lon+\",\"+circumfrence\n",
    "            coor = format(coor)\n",
    "            print()\n",
    "            print(f\"country: {country}, nearby_loc: {nearby_location},  coordinates: {lat+', '+lon}, radius: {circumfrence}\")\n",
    "            \n",
    "            try:\n",
    "                for word in nigerian_slag:\n",
    "                    count +=1\n",
    "                    \n",
    "                    print(f\"{'*'*20} num: {count};  word: {word} {'*'*20}\")\n",
    "                    scraped_tweets = sntwitter.TwitterSearchScraper(f'{str(word)} since:{start_date} until:{end_date} geocode:\"{coor}\"').get_items()\n",
    "\n",
    "                    sliced_scraped_tweets = itertools.islice(scraped_tweets, maxcounts)  \n",
    "                    df_coord = pd.DataFrame(sliced_scraped_tweets)\n",
    "                    df_coord.columns = df_coord.columns.str.lower()\n",
    "                    df_coord['keyword'] = word\n",
    "                    df_coord['nearby_location'] = nearby_location\n",
    "                    df_coord['country'] = 'country'\n",
    "                    df_x = pd.concat([df_x, df_coord])\n",
    "            except Exception as e:\n",
    "                print(f'The error was: {str(e)}')\n",
    "except Exception as e:\n",
    "    print(f'The error was: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Clean the extracted tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#subset the data\n",
    "df = df_x[['id', 'date', 'content', 'renderedcontent', 'user', 'replycount','retweetcount', 'likecount', 'quotecount',\n",
    "           'url', 'lang', 'retweetedtweet', 'quotedtweet','mentionedusers', 'coordinates', 'place', 'hashtags', 'keyword', \n",
    "           'nearby_location', 'country']]\n",
    "\n",
    "#format the date\n",
    "df['date'] = df['date'].apply(lambda x: x.strftime(\"%d-%b-%Y %H:%M:%S\"))\n",
    "\n",
    "# getting user's location\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df['username'] = df['user'].apply(lambda x: x['username'])\n",
    "df['displayname'] = df['user'].apply(lambda x: x['displayname'])\n",
    "df['user_id'] = df['user'].apply(lambda x: x['id'])\n",
    "df['user_location'] = df['user'].apply(lambda x: x['location'])\n",
    "df['description'] = df['user'].apply(lambda x: x['description'])\n",
    "df['followersCount'] = df['user'].apply(lambda x: x['followersCount'])\n",
    "df['friendsCount'] = df['user'].apply(lambda x: x['friendsCount'])\n",
    "df['statusesCount'] = df['user'].apply(lambda x: x['statusesCount'])\n",
    "df['favouritesCount'] = df['user'].apply(lambda x: x['favouritesCount'])\n",
    "df['protected'] = df['user'].apply(lambda x: x['protected'])\n",
    "\n",
    "#mentions \n",
    "pattern_1 = r'#(\\w+)'\n",
    "pattern_2 = r'@(\\w+)'\n",
    "# df['hashtag'] = df['content'].apply(lambda x: re.findall(pattern_1, x))\n",
    "df['mentions'] = df['content'].apply(lambda x: re.findall(pattern_2, x))\n",
    "\n",
    "#drop some columns \n",
    "df.drop(columns = ['user', 'mentionedusers'], inplace = True)\n",
    "\n",
    "\n",
    "#clean the data \n",
    "##lower the text\n",
    "df['content'] = df['content'].apply(lambda x: x.lower())\n",
    "df['content'] = df['content'].apply(lambda x: x.strip())\n",
    "df['content'] = df['content'].apply(lambda x: x.replace(\"  \", \" \"))\n",
    "df['content'] = df['content'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "\n",
    "##strip white spaces \n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "#lower all the names \n",
    "df.colums = df.columns.str.lower()\n",
    "\n",
    "\n",
    "#filter words that contained in the list \n",
    "df['search_word'] = np.where(df['content'].str.contains(search_word),'Yes','No')\n",
    "\n",
    "##drop duplicate tweets if any \n",
    "print(f'The number of tweets before droping duplicates are: {df.shape[0]}')\n",
    "df = df.drop_duplicates(subset=['date', 'id', 'content', 'username', 'user_location'], keep= 'last')\n",
    "print(f'The number of tweets after droping duplicates are: {df.shape[0]}')\n",
    "\n",
    "#print the data\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9bLOVKEDXlq"
   },
   "source": [
    "> Tweets containing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4XnCRT8HcGr3",
    "outputId": "bbf07f9b-145b-48a1-9d5d-cc73a48d0f3a"
   },
   "outputs": [],
   "source": [
    "df['search_word'].value_counts(normalize = True).mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drop the tweets that do not have the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "R2kFEEWfdBSn",
    "outputId": "c54483fa-41fb-4b45-ff89-a22d3b4c7703"
   },
   "outputs": [],
   "source": [
    "#filter out the only data with matching keyword\n",
    "data = df[df['search_word']!='No']\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSUXVOlwQko6"
   },
   "source": [
    "## Save the File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtWGqQFXR9wH"
   },
   "source": [
    "> Create file Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sdxdnQSMR1Ii",
    "outputId": "5cb46d1b-38f7-4b69-a6dc-97124f5692cd"
   },
   "outputs": [],
   "source": [
    "#generate a filename \n",
    "name = f\"nigerian_slang_tweets\"\n",
    "\n",
    "#get todays date\n",
    "today = datetime.now()\n",
    "\n",
    "# format the dates\n",
    "formated_date = today.strftime(\"%d_%b\").lower()\n",
    "\n",
    "#extension\n",
    "extension = \".xlsx\"\n",
    "\n",
    "#generate the name\n",
    "new_filename = name+\"_\"+formated_date\n",
    "old_filename = name+\"_\"+formated_date+\"_\"+\"oldfile\"\n",
    "print(old_filename ,\"\\n\", new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP_Zh5AwNNI5"
   },
   "source": [
    "> Write data to Postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NpsjZT01NMEa"
   },
   "outputs": [],
   "source": [
    "#create the connection \n",
    "\n",
    "db_host = 'research-dev-db.cluster-c30rc84jwwhk.us-east-2.rds.amazonaws.com'\n",
    "# db_host = '192.168.8.221'\n",
    "db_port = 5432\n",
    "db_password = 'researchpassword'\n",
    "db = 'postgres'\n",
    "db_user = 'postgres'\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db}')\n",
    "\n",
    "#write the data if the table exists , replace the data\n",
    "try:\n",
    "    data.to_sql(new_filename, engine, method=psql_insert_copy, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(f'The error is: {str(e)}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> write the african countries cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the data if the table exists , replace the data\n",
    "table_name = \"african_countries_towns_and_cities\"\n",
    "\n",
    "#write the data into db\n",
    "try:\n",
    "    coordinates_df.to_sql(table_name, engine, method=psql_insert_copy, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(f'The error is: {str(e)}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnbEXzTlSJHD"
   },
   "source": [
    "> Rename file if present in the directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3rGpqHLZSJUB",
    "outputId": "718f2953-14b5-4c98-f25b-71596d648977"
   },
   "outputs": [],
   "source": [
    "#rename the file if is in directory \n",
    "# old_filename+'.xlsx'\n",
    "# new_filename+'xlsx'\n",
    "try:\n",
    "  os.rename(old_filename+'.xlsx', new_filename+'xlsx')\n",
    "except Exception as e:\n",
    "  print(f\"The error is: {str(e)}\")\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBLNZ2TYSEBb"
   },
   "source": [
    "> Save the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1KZPXUpwQiei"
   },
   "outputs": [],
   "source": [
    "#write the file\n",
    "try:\n",
    "    data.to_excel(new_filename+'.xlsx', sheet_name = 'tweets data', index = False)\n",
    "#     coordinates_df.to_excel(\"african cities coordinates\"+'.xlsx', sheet_name = 'tweets data', index = False)\n",
    "except BaseException as e:\n",
    "    print('-'*20)\n",
    "    print(f\"The error is {str(e)}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #write the file\n",
    "# try:\n",
    "# #     data.to_excel(new_filename+'.xlsx', sheet_name = 'tweets data', index = False)\n",
    "#     coordinates_df.to_excel(\"african cities coordinates\"+'.xlsx', sheet_name = 'tweets data', index = False)\n",
    "# except BaseException as e:\n",
    "#     print('-'*20)\n",
    "#     print(f\"The error is {str(e)}\")\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSWUidVW1x8G"
   },
   "source": [
    "## Time Spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7AiXmYb1xJY"
   },
   "outputs": [],
   "source": [
    "#get the time used to proccess the data\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print(f\"The stopped at: {end_time} '\\n' Started at: {end_time} '\\n' and took {duration} to run\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Place specific tweet search : Moses Kioko.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
