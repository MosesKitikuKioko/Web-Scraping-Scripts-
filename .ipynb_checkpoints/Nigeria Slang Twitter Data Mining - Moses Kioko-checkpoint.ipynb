{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8hDA6zxpgqu"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The script automates gathering of tweet data from the archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vqWbxVU5wD2S"
   },
   "outputs": [],
   "source": [
    "#import packages \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "import requests\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#import data scraping packages \n",
    "import tweepy\n",
    "# from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import Cursor\n",
    "from tweepy import API\n",
    "\n",
    "#import web scrapping packages \n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "from selenium_stealth import stealth\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "\n",
    "\n",
    "#sns module \n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import snscrape.modules.facebook as snfacebook\n",
    "import snscrape.modules.instagram as sninstagram\n",
    "\n",
    "#postgresql database packages \n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "#check the start time\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSl4LEoodIWL"
   },
   "source": [
    "## Extract Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a4ur9LDdHju",
    "outputId": "60957d45-96d2-4e7d-cdcf-ad5abfb3f59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start date: 2021-08-22 end date is: 2022-08-22 and time duration is: 365 days, 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#generate the \n",
    "number_of_dates = 365\n",
    "end_date = date.today()\n",
    "start_date =  end_date - timedelta(days=number_of_dates)\n",
    "duration = end_date - start_date\n",
    "\n",
    "print(f\"The start date: {start_date} end date is: {end_date} and time duration is: {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwYP-zQ5IeCF",
    "outputId": "fad41d98-5e13-404b-e076-acd9d9556c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todays date:  August 22 2022 17 03 45\n"
     ]
    }
   ],
   "source": [
    "#get todays date\n",
    "today = datetime.now()\n",
    "\n",
    "# format the dates\n",
    "formated_date = today.strftime(\"%B %d %Y %H %M %S\")\n",
    "\n",
    "#print date \n",
    "print(\"Todays date: \", formated_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39bJ7aMOu_Ds"
   },
   "source": [
    "### Keywords\n",
    "\n",
    "> The following are search keywords for cyber security concerns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ySKqMbc7LbU5"
   },
   "outputs": [],
   "source": [
    "#define variables \n",
    "keywords_list = [\"Trojan\", \"Spyware\", \"ransomware\", \"Botnet\", \"Defense\", \"Evasion\",\"ingress tool transfer\", \"Malicious link\",\"firmware\", \"Bruteforce\", \"automated exfiltration\", \n",
    "                 \"DNSSEC traffic\", \"channel attacks\", \"DHCP spoofing\",\"Protocol impersonation\", \"Denial of Service\", \"Phishing\", \"Spearphishing\", \"OS Credential Dumping\"]\n",
    "\n",
    "#search words \n",
    "search_word = \"|\".join([j.lower() for j in keywords_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f9oeJDvu4F5"
   },
   "source": [
    "### African Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-saalAL9VJ7",
    "outputId": "886ecc54-7bb8-466c-d74e-27b4756f7d61"
   },
   "outputs": [],
   "source": [
    "#countries involved\n",
    "name = ['Algeria','Angola','Benin','Botswana','Burkina_Faso','Burundi','Cameroon','Cape_Verde',\n",
    "             'Central_African_Republic','Chad','Comoros','Democratic_Republic_of_the_Congo','Djibouti','Egypt',\n",
    "             'Equatorial_Guinea',' Eritrea',' Ethiopia',' Gabon',' Gambia',' Ghana','Guinea',' Guinea-Bissau',\n",
    "             'Ivory_Coast','Kenya',' Lesotho',' Liberia',' Libya',' Madagascar',' Malawi',' Mali',' Mauritania','Mauritius',\n",
    "             'Morocco',' Mozambique',' Namibia',' Niger',' Nigeria','Republic of the Congo',' Rwanda','Sao_Tome_and_Principe',\n",
    "             'Senegal',' Seychelles',' Sierra_Leone',' Somalia',' South_Africa',' Sudan','South_Sudan','Swaziland',' Tanzania',' Togo',\n",
    "             'Tunisia',' Uganda',' Zambia',' Zimbabwe']\n",
    "\n",
    "# #strip any white spaces \n",
    "country_name = [x.strip().lower().replace('_','-').replace(' ','-') for x in name]\n",
    "\n",
    "#search words \n",
    "countries = \"|\".join([j.strip().lower() for j in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "y8R4t7iRmd4R",
    "outputId": "415c399b-ca7a-4fe7-e2d9-672e2cadbb07"
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "e84__ErAko4b",
    "outputId": "40ca4129-ec3e-4472-c48c-c68ae11c2089"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moses\\AppData\\Local\\Temp\\ipykernel_3840\\1516475625.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r'C:\\Users\\chromedriver_win32\\chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    " driver = webdriver.Chrome(r'C:\\Users\\chromedriver_win32\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQvIVuH9chM1"
   },
   "source": [
    "## Place Specific \n",
    "\n",
    "> Countries towns and cities co-ordinates [link](https://www.countrycoordinate.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.countrycoordinate.com/country-algeria/\n",
      "*************** base page done ***************\n",
      "*************** Other pages ***************\n",
      "*************** pages to extract 6 ***************\n",
      "https://www.countrycoordinate.com/country-algeria/?page=2\n",
      "https://www.countrycoordinate.com/country-algeria/?page=3\n",
      "https://www.countrycoordinate.com/country-algeria/?page=4\n",
      "https://www.countrycoordinate.com/country-algeria/?page=5\n",
      "https://www.countrycoordinate.com/country-algeria/?page=12\n",
      "https://www.countrycoordinate.com/country-algeria/?page=13\n",
      "2 https://www.countrycoordinate.com/country-angola/\n",
      "*************** base page done ***************\n",
      "*************** Other pages ***************\n",
      "*************** pages to extract 1 ***************\n",
      "https://www.countrycoordinate.com/country-angola/?page=2\n",
      "3 https://www.countrycoordinate.com/country-benin/\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     13\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# get data table & save as dataframe\u001b[39;00m\n\u001b[0;32m     17\u001b[0m table_val \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m,{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable table-bordered table-hover\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\web_scraping\\lib\\site-packages\\bs4\\__init__.py:313\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    314\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    316\u001b[0m ):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#get the data in a dataframe \n",
    "coordinates_df =  pd.DataFrame()\n",
    "coordinates_df_1 = pd.DataFrame()\n",
    "count = 0\n",
    "\n",
    "for country in country_name:\n",
    "    #get the url\n",
    "    url = f\"https://www.countrycoordinate.com/country-{country}/\"\n",
    "    count +=1\n",
    "    print(count, url)     \n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    soup = bs.BeautifulSoup(driver.page_source,'lxml')\n",
    "    \n",
    "    # get data table & save as dataframe\n",
    "    table_val = soup.find(\"table\",{'table table-bordered table-hover'})\n",
    "    table_val_df = pd.read_html(str(table_val))[0]\n",
    "    \n",
    "    if len(table_val_df.columns) == 2:\n",
    "        table_val_df.columns = ['city', 'coords']\n",
    "        table_val_df['country'] = country\n",
    "        table_val_df['url'] = url\n",
    "        table_val_df[['lat', 'lon']] = table_val_df['coords'].str.split('/', expand = True)\n",
    "        table_val_df.drop(columns = ['coords'], inplace = True)\n",
    "        if table_val_df.shape[0]>1:\n",
    "            coordinates_df = pd.concat([coordinates_df,table_val_df])\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    #get the number of other pages \n",
    "    page_num_table = soup.find_all(\"table\", {\"class\":\"table table-bordered centered\"})\n",
    "\n",
    "    \n",
    " \n",
    "      #loop through other pages\n",
    "    print(f\"{'*'*15} base page done {'*'*15}\")\n",
    "    print(f\"{'*'*15} Other pages {'*'*15}\")\n",
    "    for tab in page_num_table:\n",
    "        row = tab.find_all('a')\n",
    "        row = [i.text.strip() for i in row]\n",
    "        row = row[:-1]\n",
    "        print(f\"{'*'*15} pages to extract {len(row)} {'*'*15}\")\n",
    "\n",
    "        for page in row: \n",
    "            next_page_url = url+f\"?page={page}\"\n",
    "            print(next_page_url)\n",
    "\n",
    "            #get the data \n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(2)\n",
    "            soup_page = bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            # get data table & save as dataframe\n",
    "            table_val_next = soup_page.find(\"table\",{'table table-bordered table-hover'})\n",
    "            table_val_df_1 = pd.read_html(str(table_val_next))[0]\n",
    "            if len(table_val_df_1.columns) == 2:\n",
    "                table_val_df_1.columns = ['city', 'coords']\n",
    "                table_val_df_1['country'] = country\n",
    "                table_val_df_1['url'] = url\n",
    "                table_val_df_1[['lat', 'lon']] = table_val_df_1['coords'].str.split('/', expand = True)\n",
    "                table_val_df_1.drop(columns = ['coords'], inplace = True)\n",
    "                if table_val_df_1.shape[0]>1:\n",
    "                    coordinates_df_1 = pd.concat([coordinates_df_1,table_val_df_1])\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9bLOVKEDXlq"
   },
   "source": [
    "> Loop through the dataset to get the tweets per location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QljenrEEAb-d",
    "outputId": "2021f993-69fd-491d-9f70-9238936345b2"
   },
   "outputs": [],
   "source": [
    "# maximum number of tweets per search word\n",
    "counter = 2\n",
    "\n",
    "#initial counter\n",
    "count = 0\n",
    "\n",
    "#The circumference distance in km\n",
    "distance = 100\n",
    "\n",
    "#subset the data\n",
    "#tweets temporary place holder\n",
    "tweets_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fetch the tweets\n",
    "try:\n",
    "    for row in df.index[:1]:\n",
    "        country = df['country'][row]\n",
    "        nearby_location = df['city'][row]\n",
    "        lat = df['lat'][row]\n",
    "        lon = df['lon'][row]\n",
    "        circumfrence = \"50km\"\n",
    "        location = lat+\",\"+lon+\",\"+circumfrence\n",
    "        location = format(location)\n",
    "        print()\n",
    "        print(country, nearby_location,  location)\n",
    "    \n",
    "        for count in range(counter):\n",
    "            for word in keywords_list:\n",
    "#                 print(f\"{'*'*20} The keyword is: {word} {'*'*20}\")\n",
    "                for num,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{str(word)} geocode:{loc}').get_items()): \n",
    "                    if num >= counter:\n",
    "                        break\n",
    "                    tweets_list.append([tweet.date.strftime(\"%d-%b-%Y\"), tweet.id, tweet.content, tweet.username, tweet.url, tweet.outlinksss, word, nearby_location, country])\n",
    "                    print(f\"The tweet no: {num}, keyword: {word} and link:{tweet.url}\")\n",
    "                    print(tweet.url)\n",
    "            count+=1\n",
    "            break\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'The error was: {str(e)}')\n",
    "    pass\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "data = pd.DataFrame(tweets_list, columns=['date', 'Tweet_id', 'Text',  'username', 'tweet_url', 'tweet.outlinksss', 'keyword', 'nearby_location', 'country'])\n",
    "\n",
    "# #lower text\n",
    "# #lower the text\n",
    "# data['Text'] = df['Text'].apply(lambda x: x.lower())\n",
    "# data['Text'] = df['Text'].apply(lambda x: x.strip())\n",
    "# data['Text'] = df['Text'].apply(lambda x: x.replace(\"  \", \" \"))\n",
    "# data['Text'] = df['Text'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "\n",
    "# #strip white spaces \n",
    "# data.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "# #filter words that contained in the list \n",
    "# data['search_word'] = np.where(data['Text'].str.contains(search_word),'Yes','No')\n",
    "\n",
    "# #drop duplicate tweets if any \n",
    "# print(f'The number of tweets before droping duplicates are: {data.shape[0]}')\n",
    "# data = df.drop_duplicates(subset=['date', 'Tweet_id', 'Text', 'username'], keep= 'last')\n",
    "# print(f'The number of tweets after droping duplicates are: {data.shape[0]}')\n",
    "\n",
    "# # Display first 5 entries from dataframe\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4XnCRT8HcGr3",
    "outputId": "bbf07f9b-145b-48a1-9d5d-cc73a48d0f3a"
   },
   "outputs": [],
   "source": [
    "df['search_word'].value_counts(normalize = True).mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "R2kFEEWfdBSn",
    "outputId": "c54483fa-41fb-4b45-ff89-a22d3b4c7703"
   },
   "outputs": [],
   "source": [
    "#filter out the only data with matching keyword\n",
    "df = df[df['search_word']!='No']\n",
    "df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSUXVOlwQko6"
   },
   "source": [
    "## Save the File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtWGqQFXR9wH"
   },
   "source": [
    "> Create file Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sdxdnQSMR1Ii",
    "outputId": "5cb46d1b-38f7-4b69-a6dc-97124f5692cd"
   },
   "outputs": [],
   "source": [
    "#generate a filename \n",
    "name = f\"snsscrape_global_tweets\"\n",
    "\n",
    "#get todays date\n",
    "today = datetime.now()\n",
    "\n",
    "# format the dates\n",
    "formated_date = today.strftime(\"%d_%b\").lower()\n",
    "\n",
    "#extension\n",
    "extension = \".xlsx\"\n",
    "\n",
    "#generate the name\n",
    "new_filename = name+\"_\"+formated_date\n",
    "old_filename = name+\"_\"+formated_date+\"_\"+\"oldfile\"\n",
    "print(old_filename ,\"\\n\", new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP_Zh5AwNNI5"
   },
   "source": [
    "> Write data to Postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NpsjZT01NMEa"
   },
   "outputs": [],
   "source": [
    "#create the connection \n",
    "db_host = 'research-dev-db.cluster-c30rc84jwwhk.us-east-2.rds.amazonaws.com'\n",
    "# db_host = '192.168.8.221'\n",
    "db_port = 5432\n",
    "db_password = 'researchpassword'\n",
    "db = 'postgres'\n",
    "db_user = 'postgres'\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db}')\n",
    "\n",
    "#write the data if the table exists , replace the data\n",
    "try:\n",
    "    df.to_sql(new_filename, engine, method=psql_insert_copy, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(f'The error is: {str(e)}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnbEXzTlSJHD"
   },
   "source": [
    "> Rename file if present in the directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3rGpqHLZSJUB",
    "outputId": "718f2953-14b5-4c98-f25b-71596d648977"
   },
   "outputs": [],
   "source": [
    "#rename the file if is in directory \n",
    "# old_filename+'.xlsx'\n",
    "# new_filename+'xlsx'\n",
    "try:\n",
    "  os.rename(old_filename+'.xlsx', new_filename+'xlsx')\n",
    "except Exception as e:\n",
    "  print(f\"The error is: {str(e)}\")\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBLNZ2TYSEBb"
   },
   "source": [
    "> Save the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1KZPXUpwQiei"
   },
   "outputs": [],
   "source": [
    "#write the file\n",
    "try:\n",
    "  df.to_excel(new_filename+'.xlsx', sheet_name = 'tweets data', index = False)\n",
    "except BaseException as e:\n",
    "  print('-'*20)\n",
    "  print(f\"The error is {str(e)}\")\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSWUidVW1x8G"
   },
   "source": [
    "## Time Spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7AiXmYb1xJY"
   },
   "outputs": [],
   "source": [
    "#get the time used to proccess the data\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print(f\"The stopped at: {end_time} '\\n' Started at: {end_time} '\\n' and took {duration} to run\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Place specific tweet search : Moses Kioko.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
